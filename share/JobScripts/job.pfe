#!/bin/csh

# Job script for Pleiades computer at NASA Ames.
#
# This file must be customized and run from the run directory! For example
#
#   cd run
#   qsub job.pfe
#
# An alternative is to use the qsub.pfe.pl and watch.pfe.pl scripts 
# for the normal/long queues. Use -h for help on usage.
# 
# Note that the run directory should be moved to the /nobackup filesystem,
# otherwise the quota on the home directory will fill up before the run ends.
# Even on the /nobackup system there is a limit on the number of files.
# To see the disk quota limits and usage use
#
# lfs quota -u USERNAME /nobackupp1
#
# To avoid having too many output files run
#
#    PostProc.pl -g -r=60 >& PostProc.log &
#
# on the head node (post-processes every minute). 
#
# Also use (in the BATSRUS section of the PARAM.in file) the command
#
# #RESTARTOUTFILE
# one                             TypeRestartOutFile
#
# to reduce the number of files in the restart directory.
#
# To see the CPU allocation and usage on your accounts use
#
# acct_ytd
#
# For detailed information for a period of time, project and user(s):
#
# acct_query -b 10/31/15 -p s1359 -u all
#
# To see the number of free nodes per machine type and queue use
#
# node_stats.sh
#
# This is very useful before submitting a job to the devel queue.

#PBS -S /bin/csh
#PBS -N SWMF

# set the number of MPI processes by changing select and ncpus:
# nProc = select*ncpus

# To run on the 16-core Sandy Bridge nodes (2GB/core)
#PBS -lselect=16:ncpus=16:model=san

# To run on the 20-core Ivy Bridge nodes (3.2GB/core)
### PBS -l select=16:ncpus=20:model=ivy

# To run on the 24-core Haswell nodes (5.3GB/core)
### PBS -l select=16:ncpus=24:model=has

# To run on the 28-core Broadwell nodes (128GB/node or 4.5GB/core)
### PBS -l select=16:ncpus=28:model=bro

# To run on the 28-core Electra Broadwell nodes (128GB/node or 4.5GB/core)
### PBS -l select=16:ncpus=28:model=bro_ele

# The default is the "normal" queue with an 8 hour walltime limit.

# To run in the long queue uncomment the following line,
# and set the maximum walltime to up to 120 hours below. 
### PBS -q long

# To run in the development queue uncomment the following line,
# and set the maximum walltime to 2 hours below. 
# Note that self-submitting job scripts are not allowed anymore!
### PBS -q devel

#PBS -l walltime=8:00:00
#PBS -j oe
#PBS -m e

# Specify group (account charged) if necessary
### PBS -W group_list=...

# cd into the run directory
cd $PBS_O_WORKDIR

# These settings may or may not be useful
#setenv MPI_MSGS_PER_HOST 100000
#setenv MPI_MSGS_PER_PROC 100000
#setenv MPI_MSGS_MAX      100000

# Seems to be needed for HDF5 plots
setenv MPI_TYPE_DEPTH 20

# run SWMF (the number of processors is already specified above)
# the date/time stamp for runlog is only necessary for automated resubmission
mpiexec ./SWMF.exe > runlog_`date +%y%m%d%H%M`

exit

# To use automated resubmission remove the 'exit' command above
# and adapt the script below!
#
# Use the #CPUTIMEMAX and #CHECKSTOP commands in PARAM.in
# so the code stops before the wall clock time is exceeded.

# Do not continue unless the job finished successfully
if(! -f SWMF.SUCCESS) exit

# Do not continue if the whole run is done
if(-f SWMF.DONE) exit

# Link latest restart files
./Restart.pl

# Provide a PARAM.in.restart file if you wish and uncomment these lines:
# if(! -f PARAM.in.start) cp PARAM.in PARAM.in.start
# if(-f PARAM.in.restart) cp PARAM.in.restart PARAM.in

# Resubmit job
qsub job.pfe
